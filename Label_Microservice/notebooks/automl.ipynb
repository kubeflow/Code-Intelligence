{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using AutoML\n",
    "\n",
    "* This notebook uses [AutoML](https://cloud.google.com/natural-language/automl/docs/tutorial#step_1_create_a_dataset) to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Whether to wait for the model to be trained and then\n",
    "# deploy it.\n",
    "deploy = False\n",
    "max_age_days = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding /home/jovyan/git_kubeflow-code-intelligence/py to python path\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "import sys\n",
    "import notebook_setup\n",
    "\n",
    "notebook_setup.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess \n",
    "# TODO(jlewi): Get the project using fairing?\n",
    "# PROJECT = subprocess.check_output([\"gcloud\", \"config\", \"get-value\", \"project\"]).strip().decode()\n",
    "PROJECT = \"issue-label-bot-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-automl in /home/jovyan/.local/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-cloud-automl) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (1.51.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (1.14.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (3.11.3)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (46.1.3)\n",
      "Requirement already satisfied: pytz in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (2019.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (1.13.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.8.2; extra == \"grpc\" in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (1.26.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jovyan/.local/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jovyan/.local/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/.local/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/jovyan/.local/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jovyan/.local/lib/python3.6/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (0.2.5)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/jovyan/.local/lib/python3.6/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /home/jovyan/.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-automl) (0.4.5)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -r ../requirements.train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the AutoML dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset name: projects/976279526634/locations/us-central1/datasets/TCN4282013949513170944\n",
      "Dataset id: TCN4282013949513170944\n"
     ]
    }
   ],
   "source": [
    "# TODO(jlewi): How do we check if the dataset already exists and whether it already has data\n",
    "from google.cloud import automl\n",
    "import logging\n",
    "\n",
    "display_name = \"kubeflow_issues_with_repo\"\n",
    "\n",
    "client = automl.AutoMlClient()\n",
    "\n",
    "# A resource that represents Google Cloud Platform location.\n",
    "project_location = client.location_path(PROJECT, \"us-central1\")\n",
    "# Specify the classification type\n",
    "# Types:\n",
    "# MultiLabel: Multiple labels are allowed for one example.\n",
    "# MultiClass: At most one label is allowed per example.\n",
    "metadata = automl.types.TextClassificationDatasetMetadata(\n",
    "    classification_type=automl.enums.ClassificationType.MULTILABEL\n",
    ")\n",
    "dataset = automl.types.Dataset(\n",
    "    display_name=display_name,\n",
    "    text_classification_dataset_metadata=metadata,\n",
    ")\n",
    "\n",
    "# Create a dataset with the dataset metadata in the region.\n",
    "response = client.create_dataset(project_location, dataset)\n",
    "\n",
    "created_dataset = response.result()\n",
    "\n",
    "# Display the dataset information\n",
    "logging.info(\"Dataset name: {}\".format(created_dataset.name))\n",
    "dataset_id = created_dataset.name.split(\"/\")[-1]\n",
    "logging.info(f\"Dataset id: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "* [Docs](https://cloud.google.com/natural-language/automl/docs/prepare) for preparing the dataset\n",
    "* We need to create a CSV file that lists all the data files\n",
    "* We need to upload each document as a text file to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Elapsed 7.0 s. Waiting...\n",
      "  Elapsed 8.09 s. Waiting...\n",
      "  Elapsed 9.19 s. Waiting...\n",
      "  Elapsed 10.27 s. Waiting...\n",
      "  Elapsed 11.28 s. Waiting...\n",
      "  Elapsed 12.35 s. Waiting...\n",
      "  Elapsed 13.45 s. Waiting...\n",
      "  Elapsed 14.54 s. Waiting...\n",
      "Downloading: 100%|██████████| 11259/11259 [00:03<00:00, 3600.93rows/s]\n",
      "Total time taken 18.49 s.\n",
      "Finished at 2020-06-28 17:15:40.\n"
     ]
    }
   ],
   "source": [
    "from code_intelligence import github_bigquery\n",
    "recent_issues = github_bigquery.get_issues(\"kubeflow\", PROJECT, max_age_days=max_age_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use a bucket in the same region and type as automl\n",
    "data_dir = f\"gs://issue-label-bot-dev_automl/automl_{dataset_id}\"\n",
    "issues_dir = os.path.join(data_dir, \"issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_intelligence import gcs_util\n",
    "from code_intelligence import github_util\n",
    "from code_intelligence import util\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "info = pd.DataFrame(columns=[\"url\", \"set\", \"labels\"], index=range(recent_issues.shape[0]))\n",
    "\n",
    "# Make the set an empty string because we will let AutoML assign points to the train, eval and test sets\n",
    "info[\"set\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_arena_316.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_arena_317.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_131.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_132.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_133.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_135.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_136.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_137.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_139.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_140.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_141.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_code-intelligence_142.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_86.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_89.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_92.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_94.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_96.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_97.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_98.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_common_99.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community-infra_2.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community-infra_4.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community-infra_6.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community_336.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community_338.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community_340.txt\n",
      "Created gs://issue-label-bot-dev_automl/automl_TCN4282013949513170944/issues/kubeflow_community_341.txt\n"
     ]
    }
   ],
   "source": [
    "storage_client = storage.Client()\n",
    "\n",
    "bucket_name, _ = gcs_util.split_gcs_uri(data_dir)\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "for i in range(recent_issues.shape[0]):\n",
    "    owner, repo, number = util.parse_issue_url(recent_issues.iloc[i][\"html_url\"])\n",
    "    owner_repo = f\"{owner}_{repo}\"\n",
    "    name = f\"{owner}_{repo}_{number}.txt\"\n",
    "    target = os.path.join(issues_dir, name)\n",
    "\n",
    "    issue = recent_issues.iloc[i]\n",
    "    \n",
    "    if gcs_util.check_gcs_object(target, storage_client=storage_client):\n",
    "        logging.info(f\"{target} already exists\")\n",
    "        \n",
    "    else:\n",
    "        _, obj_path = gcs_util.split_gcs_uri(target)\n",
    "        blob = bucket.blob(obj_path)\n",
    "        \n",
    "        # Include the owner and repo in the text body because it is predictive\n",
    "        doc = github_util.build_issue_doc(owner, repo, issue[\"title\"], [issue[\"body\"]])\n",
    "        blob.upload_from_string(doc)\n",
    "        logging.info(f\"Created {target}\")\n",
    "\n",
    "    info.iloc[i][\"url\"] = target    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the CSV file with the data\n",
    "* We don't use pandas to_csv because this ends up putting quoting the string containing the labels e.g\n",
    "\n",
    "  ```\n",
    "  ,gs://issue-label-bot-dev/automl_2020_0429/issues/kubeflow_website_997.txt,\"area/docs, kind/feature, lifecycle/stale, priority/p2\"\n",
    "  ```\n",
    "* But that isn't the format AutoML expects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Target Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a historgram of label frequency\n",
    "\n",
    "* AutoML requires labels have a minimum count of each label (8 for training, 1 for validation, 1 for test) so filter out labels that don't appear very often\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "label_counts = Counter()\n",
    "\n",
    "for r in range(recent_issues.shape[0]):\n",
    "    label_counts.update(recent_issues.iloc[r][\"parsed_labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_counts_df = pd.DataFrame({\"label\": label_counts.keys(), \"count\": label_counts.values()})\n",
    "label_counts_df = pd.DataFrame(label_counts.items(), columns=[\"label\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_df.sort_values(\"count\", ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 50\n",
    "target_labels = label_counts_df.loc[label_counts_df[\"count\"] > cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguish unlabeled vs. negative examples\n",
    "\n",
    "* We need to distinguish between unlabeled examples and negative examples\n",
    "* For example, if an issue doesn't have label \"platform/gcp\" that could be for one of two reasons\n",
    "  1. The issue was never labeled\n",
    "  1. The label platform/gcp doesn't apply\n",
    "  \n",
    "* A quick hack to distinguish the two is to only include area and platform labels\n",
    "\n",
    "  * For now at least if one of these labels exists on an issue it was probably applied by a human\n",
    "  * This is in contrast to kind labels which could be applied by the bot or by a GitHub issue template\n",
    "  \n",
    "* Longer term we could look at GitHub events to infer whether data was labeled by a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = target_labels[target_labels[\"label\"].apply(lambda x: x.startswith(\"area\") or x.startswith(\"platform\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter labels to target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_filter(labels):\n",
    "    filtered = []\n",
    "    for l in labels:\n",
    "        if l in target_labels.values:\n",
    "            filtered.append(l)\n",
    "    return filtered\n",
    "\n",
    "info[\"labels\"] = recent_issues[\"parsed_labels\"].apply(label_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute string for automl\n",
    "\n",
    "# AutoML doesn't allow \"/\" only letters, dashes, underscores are allowed in labels\n",
    "# We need a comma separated string and we need to replace \"/\" with \"-\"\n",
    "info[\"automl_labels\"] = info[\"labels\"].apply(lambda l: \", \".join(l).replace(\"/\", \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import csv\n",
    "buffer = io.StringIO()\n",
    "\n",
    "# AutoML seems to require at least 1 label for every issue\n",
    "#labeled_rows = info.loc[info[\"labels\"] != \"\"]\n",
    "#labeled_rows = info.loc[info[\"labels\"] != \"\"]\n",
    "#labeled_rows.to_csv(buffer, columns=[\"set\", \"url\", \"labels\"], header=False, index=False)\n",
    "\n",
    "info.to_csv(buffer, columns=[\"set\", \"url\", \"automl_labels\"], header=False, index=False, doublequote=False)\n",
    "\n",
    "# for i in range(labeled_rows.shape[0]):\n",
    "#     row = labeled_rows.iloc[i]    \n",
    "#     buffer.write(f\"{row['set']}, {row['url']}, {row['labels']}\\n\")\n",
    "    \n",
    "now = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "dataset_path = os.path.join(data_dir, f\"dataset_{now}.csv\")\n",
    "_, obj_path = gcs_util.split_gcs_uri(dataset_path)\n",
    "blob = bucket.blob(obj_path)\n",
    "\n",
    "blob.upload_from_string(buffer.getvalue())\n",
    "\n",
    "logging.info(f\"Created {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the data to AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import automl\n",
    "\n",
    "dataset_full_id = client.dataset_path(\n",
    "    PROJECT, \"us-central1\", dataset_id\n",
    ")\n",
    "\n",
    "# Get the multiple Google Cloud Storage URIs\n",
    "input_uris = [dataset_path]\n",
    "gcs_source = automl.types.GcsSource(input_uris=input_uris)\n",
    "input_config = automl.types.InputConfig(gcs_source=gcs_source)\n",
    "# Import data from the input URI\n",
    "response = client.import_data(dataset_full_id, input_config)\n",
    "\n",
    "logging.info(f\"Processing import: operation: {response.operation.name}\")\n",
    "\n",
    "# This appears to be a blocking call\n",
    "logging.info(\"Data imported. {}\".format(response.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A resource that represents Google Cloud Platform location.\n",
    "project_location = client.location_path(PROJECT, \"us-central1\")\n",
    "# Leave model unset to use the default base model provided by Google\n",
    "metadata = automl.types.TextClassificationModelMetadata()\n",
    "model = automl.types.Model(\n",
    "    display_name=display_name,\n",
    "    dataset_id=dataset_id,\n",
    "    text_classification_model_metadata=metadata,\n",
    ")\n",
    "\n",
    "# Create a model with the model metadata in the region.\n",
    "response = client.create_model(project_location, model)\n",
    "\n",
    "print(u\"Training operation name: {}\".format(response.operation.name))\n",
    "print(\"Training started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model\n",
    "\n",
    "* We need to deploy the model before we can send predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a value like \"projects/976279526634/locations/us-central1/models/TCN654213816573231104'\"\n",
    "# This is blocking\n",
    "if deploy:\n",
    "    result = response.result()\n",
    "    model_name = result.name\n",
    "    deploy_response = client.deploy_model(model_name)\n",
    "    final_response = deploy_response.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deploy:\n",
    "    prediction_client = automl.PredictionServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deploy:\n",
    "    text_snippet = automl.types.TextSnippet(\n",
    "        content=\"tfjob isn't working. I can't run my training jobs\", mime_type=\"text/plain\"\n",
    "    )\n",
    "    payload = automl.types.ExamplePayload(text_snippet=text_snippet)\n",
    "\n",
    "    response = prediction_client.predict(model_name, payload)\n",
    "\n",
    "    for annotation_payload in response.payload:\n",
    "        print(\n",
    "            u\"Predicted class name: {}\".format(annotation_payload.display_name)\n",
    "        )\n",
    "        print(\n",
    "            u\"Predicted class score: {}\".format(\n",
    "                annotation_payload.classification.score\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
