{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Kubeflow Pipelines to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry point using fairing\n",
    "Kubeflow [Fairing](https://www.kubeflow.org/docs/fairing/) is a Python package that makes training and deploying machine learning models on Kubeflow easier.\n",
    "\n",
    "Here, we use the preprocessor in Kubeflow Fairing to convert a notebook to be a Python script and create an entry point for that script. After preprocessing the notebook, we can call the command in the command line like the following to run\n",
    "```Python\n",
    "$ python repo_mlp.py train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('issues_loader.py'),\n",
       " 'embeddings.py',\n",
       " 'repo_config.py',\n",
       " 'inference.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('IssuesLoader', notebook_file='issues_loader.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['embeddings.py', 'inference.py', 'repo_config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('repo_mlp.py'), 'mlp.py', 'repo_config.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP', notebook_file='repo_mlp.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'repo_config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Fairing to build docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fairing\n",
    "from fairing.builders import append\n",
    "from fairing.builders import cluster\n",
    "from fairing.deployers import job\n",
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue-label-bot-dev\n",
      "gcr.io/issue-label-bot-dev/training\n"
     ]
    }
   ],
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "print(GCP_PROJECT)\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/training'.format(GCP_PROJECT)\n",
    "print(DOCKER_REGISTRY)\n",
    "PY_VERSION = \".\".join([str(x) for x in sys.version_info[0:3]])\n",
    "BASE_IMAGE = 'python:{}'.format(PY_VERSION)\n",
    "# ucan use Dockerfile in this repo to build and use the base_image\n",
    "base_image = 'gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image\n",
    "We use builders in Kubeflow Fairing to build docker images. We use `ClusterBuilder` to builds a docker image in a Kubernetes cluster and `AppendBuilder` to append a new layer tarball. We also include `preprocessor` as a parameter to send the processed inputs to docker build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('repo_mlp.py'),\n",
       " 'issues_loader.py',\n",
       " 'inference.py',\n",
       " 'mlp.py',\n",
       " 'embeddings.py',\n",
       " 'repo_config.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP', notebook_file='repo_mlp.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'repo_config.py', 'embeddings.py', 'inference.py', 'issues_loader.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building image using cluster builder.\n",
      "Creating docker context: /tmp/fairing_context_sra1lc_s\n",
      "Waiting for fairing-builder-6vlb9 to start...\n",
      "Waiting for fairing-builder-6vlb9 to start...\n",
      "Waiting for fairing-builder-6vlb9 to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6\n",
      "\u001b[33mWARN\u001b[0m[0006] Error while retrieving image from cache: getting image from path: open /cache/sha256:007490fe99543d64363755e69ed47047c45406ae163a30fab2a7a55ec3710ceb: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0007] Checking for cached layer gcr.io/issue-label-bot-dev/training/fairing-job/cache:5abd94715d7d7183ae347a324cf72f4902c76c64083014668bdc256c7df55814...\n",
      "\u001b[36mINFO\u001b[0m[0007] No cached layer found for cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0007] Unpacking rootfs as cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi requires it.\n",
      "\u001b[36mINFO\u001b[0m[0172] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0231] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0231] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0231] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0231] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0231] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0231] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0289] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0289] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0289] Changed working directory to /app/\n",
      "\u001b[36mINFO\u001b[0m[0289] Creating directory /app/\n",
      "\u001b[36mINFO\u001b[0m[0289] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0289] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0289] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0289] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0289] cmd: /bin/bash\n",
      "\u001b[36mINFO\u001b[0m[0289] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "\u001b[36mINFO\u001b[0m[0289] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0305] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0305] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0305] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0305] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0305] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0306] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0326] No files were changed, appending empty layer to config. No layer added to image.\n",
      "\u001b[36mINFO\u001b[0m[0326] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0326] Pushing layer gcr.io/issue-label-bot-dev/training/fairing-job/cache:5abd94715d7d7183ae347a324cf72f4902c76c64083014668bdc256c7df55814 to cache now\n",
      "\u001b[36mINFO\u001b[0m[0326] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0326] Taking snapshot of files...\n",
      "2019/08/02 17:20:14 existing blob: sha256:89732bc7504122601f40269fc9ddfb70982e633ea9caf641ae45736f2846b004\n",
      "2019/08/02 17:20:14 existing blob: sha256:d68afcc1cf651f4109698bde41c1173e6c1968c3a15b3cd13430d6bcde8ed8b4\n",
      "2019/08/02 17:20:14 gcr.io/issue-label-bot-dev/training/fairing-job/cache:5abd94715d7d7183ae347a324cf72f4902c76c64083014668bdc256c7df55814: digest: sha256:ff33c06878a7f553ac692b92ededd58e3be8bf3434a386c410726bc17fe3bfbd size: 423\n",
      "2019/08/02 17:20:14 existing blob: sha256:421e23cecfe86d864c15dc6cf8c6f41b9b8f356497d45fa786a7d3906b0fd38b\n",
      "2019/08/02 17:20:14 existing blob: sha256:8645605407042d2d2a5dc382ca9be54bca87eae1d1e5b97ccd4dff54d9b72207\n",
      "2019/08/02 17:20:14 existing blob: sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2\n",
      "2019/08/02 17:20:14 existing blob: sha256:4bc7ff0b2218b70a30d625db3516f88c4820879fdf232f8ad7b471cb7d13c90f\n",
      "2019/08/02 17:20:14 existing blob: sha256:5b6ac7f35d3dce1cd93b9740c7342fe317b179c5ebb8d9e92069d5c671a756ae\n",
      "2019/08/02 17:20:14 existing blob: sha256:f401bdaa92adf9a46956b65a2d86021969297b5a5ed17b9820a54c8e6d44cf85\n",
      "2019/08/02 17:20:14 existing blob: sha256:6669e38ab1bac2750ee43e9fd769478543e5ee6866cfbbe10c5f55584681792d\n",
      "2019/08/02 17:20:14 existing blob: sha256:1806ed0db5a38109f6c2b4be674030cd8b837e6ab8e5fe579358072541410777\n",
      "2019/08/02 17:20:14 existing blob: sha256:0809e577f6d6c969247b35a338c0809b324e29ee01964de6dd3c3fe4808b2a15\n",
      "2019/08/02 17:20:14 existing blob: sha256:d5c73556cc1e31575beb68d582444652a0e1d704e0a19bbab8565e777493ece0\n",
      "2019/08/02 17:20:14 existing blob: sha256:3339a46501ea8e79588cf6baaf5a177ea871596dfc9ffd82c93fafbdabdf97f4\n",
      "2019/08/02 17:20:14 existing blob: sha256:cecc8221de0060e5aa983adaaef55f773f8c9e8cc48fe98ea66b9475a6b67e3d\n",
      "2019/08/02 17:20:14 existing blob: sha256:cbeb255d6ab1895a17ac078089e678fa0189fd8204c85ad79ab69a766dbb2e8a\n",
      "2019/08/02 17:20:14 existing blob: sha256:c39370e9b2806fb7774e4aef6905138a37d4e070efceca9b64830cc973c75bf4\n",
      "2019/08/02 17:20:14 existing blob: sha256:d801c0bd10cff9d745329edcab5c0ecd7101398438ed0eadcc8ff3aa99dcad43\n",
      "2019/08/02 17:20:15 existing blob: sha256:ebcc3afca65a753c949ed25905b3942bd548091a864171ce296ea4ee89344d8c\n",
      "2019/08/02 17:20:15 existing blob: sha256:9b879c086db6bc851a1778df6fb173a87e30a08b34810add3ffcd8b7103510d4\n",
      "2019/08/02 17:20:15 existing blob: sha256:c011ade78a64c3bef40c7a4cd87d4274a4808f7826c0b70f6679205931593cc1\n",
      "2019/08/02 17:20:15 existing blob: sha256:e4732fdd9b394aca10fbc52eb5dbd8c5a3b04e7c99044a7fd44cca2b28774f4b\n",
      "2019/08/02 17:20:15 existing blob: sha256:da348912572d5e12d8c7e54b6a26417c5a2a168238c3a1c961be1ae2363eb2b8\n",
      "2019/08/02 17:20:15 existing blob: sha256:a5abf09960671b05108c96974fb37583588b286621ccd3b1229ee948d23af04e\n",
      "2019/08/02 17:20:15 existing blob: sha256:288a4fedee6334161135d6d6c9cb91211a5f755693375bc051be4b3c2a1f4c99\n",
      "2019/08/02 17:20:15 existing blob: sha256:1fa46105e1876c0cc24a92da5cbacd7b4a09d22675a6fd7001cce1cad8493a84\n",
      "2019/08/02 17:20:15 existing blob: sha256:e059dd98ac7cff88cacd4e01f2f1d56af872618aac98b0aff8afdd81b9fd2c76\n",
      "2019/08/02 17:20:15 existing blob: sha256:9ca2ffc4c4140b428536626739a4503c91cdff680a606b8556c1e648bb113eb6\n",
      "2019/08/02 17:20:15 existing blob: sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0\n",
      "2019/08/02 17:20:15 existing blob: sha256:13806a5c262364d945866dc42e584c11259483e90e367e20d7f9ff2bc9401aed\n",
      "2019/08/02 17:20:15 existing blob: sha256:c700fb5e8860d28622a0214639aee6bdc260c8769bb688e9a6a84765b9350551\n",
      "2019/08/02 17:20:15 existing blob: sha256:d718e4299c08d3ee1aca64bdc55d04f86142879fa278eae6c2adaa6aee273598\n",
      "2019/08/02 17:20:15 existing blob: sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f\n",
      "2019/08/02 17:20:16 pushed blob sha256:06d60fe22e064179bd3ee6c30fe9d6a78c6746dc1f46eed99f54751482dc9aed\n",
      "2019/08/02 17:20:16 pushed blob sha256:f43002c5b4786c06f7bc42dc396b1fc1a7b24141f332369e1ab3c2e8c713db39\n",
      "2019/08/02 17:20:16 pushed blob sha256:ce4a5fbdbd4f0d4c47948c5edba2181978070e9c8e0167cf0efc070e14de2224\n",
      "2019/08/02 17:20:16 pushed blob sha256:713c3d8d652308adf0760e979bd391154bf2c93cc5c2430465f1c62b7b02c418\n",
      "2019/08/02 17:20:17 gcr.io/issue-label-bot-dev/training/fairing-job:E86BC577: digest: sha256:2335167ba5c12a9847dd526d4eb1c3f340a5118cde5fea85a8fe15ad93d63ca6 size: 5631\n"
     ]
    }
   ],
   "source": [
    "cluster_builder = cluster.cluster.ClusterBuilder(registry=DOCKER_REGISTRY,\n",
    "                                                 base_image=base_image,\n",
    "                                                 namespace='chunhsiang',\n",
    "                                                 preprocessor=preprocessor,\n",
    "                                                 pod_spec_mutators=[fairing.cloud.gcp.add_gcp_credentials_if_exists],\n",
    "                                                 context_source=cluster.gcs_context.GCSContextSource())\n",
    "cluster_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_v7acjf8b\n",
      "repo_mlp.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'gcr.io/issue-label-bot-dev/training/fairing-job:E86BC577'\n",
      "Invoking 'docker-credential-gcr' to obtain Docker credentials.\n",
      "Successfully obtained Docker credentials.\n",
      "Image successfully built in 2.1809086540015414s.\n",
      "Pushing image gcr.io/issue-label-bot-dev/training/fairing-job:5307C7E7...\n",
      "Loading Docker credentials for repository 'gcr.io/issue-label-bot-dev/training/fairing-job:5307C7E7'\n",
      "Invoking 'docker-credential-gcr' to obtain Docker credentials.\n",
      "Successfully obtained Docker credentials.\n",
      "Uploading gcr.io/issue-label-bot-dev/training/fairing-job:5307C7E7\n",
      "Layer sha256:ce4a5fbdbd4f0d4c47948c5edba2181978070e9c8e0167cf0efc070e14de2224 exists, skipping\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:288a4fedee6334161135d6d6c9cb91211a5f755693375bc051be4b3c2a1f4c99 exists, skipping\n",
      "Layer sha256:3339a46501ea8e79588cf6baaf5a177ea871596dfc9ffd82c93fafbdabdf97f4 exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:9b879c086db6bc851a1778df6fb173a87e30a08b34810add3ffcd8b7103510d4 exists, skipping\n",
      "Layer sha256:06d60fe22e064179bd3ee6c30fe9d6a78c6746dc1f46eed99f54751482dc9aed exists, skipping\n",
      "Layer sha256:1806ed0db5a38109f6c2b4be674030cd8b837e6ab8e5fe579358072541410777 exists, skipping\n",
      "Layer sha256:13806a5c262364d945866dc42e584c11259483e90e367e20d7f9ff2bc9401aed exists, skipping\n",
      "Layer sha256:f401bdaa92adf9a46956b65a2d86021969297b5a5ed17b9820a54c8e6d44cf85 exists, skipping\n",
      "Layer sha256:6669e38ab1bac2750ee43e9fd769478543e5ee6866cfbbe10c5f55584681792d exists, skipping\n",
      "Layer sha256:713c3d8d652308adf0760e979bd391154bf2c93cc5c2430465f1c62b7b02c418 exists, skipping\n",
      "Layer sha256:c011ade78a64c3bef40c7a4cd87d4274a4808f7826c0b70f6679205931593cc1 exists, skipping\n",
      "Layer sha256:9ca2ffc4c4140b428536626739a4503c91cdff680a606b8556c1e648bb113eb6 exists, skipping\n",
      "Layer sha256:d718e4299c08d3ee1aca64bdc55d04f86142879fa278eae6c2adaa6aee273598 exists, skipping\n",
      "Layer sha256:cbeb255d6ab1895a17ac078089e678fa0189fd8204c85ad79ab69a766dbb2e8a exists, skipping\n",
      "Layer sha256:d5c73556cc1e31575beb68d582444652a0e1d704e0a19bbab8565e777493ece0 exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:d801c0bd10cff9d745329edcab5c0ecd7101398438ed0eadcc8ff3aa99dcad43 exists, skipping\n",
      "Layer sha256:4bc7ff0b2218b70a30d625db3516f88c4820879fdf232f8ad7b471cb7d13c90f exists, skipping\n",
      "Layer sha256:c39370e9b2806fb7774e4aef6905138a37d4e070efceca9b64830cc973c75bf4 exists, skipping\n",
      "Layer sha256:0809e577f6d6c969247b35a338c0809b324e29ee01964de6dd3c3fe4808b2a15 exists, skipping\n",
      "Layer sha256:421e23cecfe86d864c15dc6cf8c6f41b9b8f356497d45fa786a7d3906b0fd38b exists, skipping\n",
      "Layer sha256:5b6ac7f35d3dce1cd93b9740c7342fe317b179c5ebb8d9e92069d5c671a756ae exists, skipping\n",
      "Layer sha256:da348912572d5e12d8c7e54b6a26417c5a2a168238c3a1c961be1ae2363eb2b8 exists, skipping\n",
      "Layer sha256:1fa46105e1876c0cc24a92da5cbacd7b4a09d22675a6fd7001cce1cad8493a84 exists, skipping\n",
      "Layer sha256:ebcc3afca65a753c949ed25905b3942bd548091a864171ce296ea4ee89344d8c exists, skipping\n",
      "Layer sha256:e059dd98ac7cff88cacd4e01f2f1d56af872618aac98b0aff8afdd81b9fd2c76 exists, skipping\n",
      "Layer sha256:8645605407042d2d2a5dc382ca9be54bca87eae1d1e5b97ccd4dff54d9b72207 exists, skipping\n",
      "Layer sha256:c700fb5e8860d28622a0214639aee6bdc260c8769bb688e9a6a84765b9350551 exists, skipping\n",
      "Layer sha256:a5abf09960671b05108c96974fb37583588b286621ccd3b1229ee948d23af04e exists, skipping\n",
      "Layer sha256:e4732fdd9b394aca10fbc52eb5dbd8c5a3b04e7c99044a7fd44cca2b28774f4b exists, skipping\n",
      "Layer sha256:cecc8221de0060e5aa983adaaef55f773f8c9e8cc48fe98ea66b9475a6b67e3d exists, skipping\n",
      "Layer sha256:a2f7363b4a9ad40514c245d5f9817ff97af66e39b2e5b21d4af7905388aa79eb pushed.\n",
      "Layer sha256:b38a4562fdbfdd1c861f4b9aee476b97e34403fadba5e29da7cbe48fb35498f4 pushed.\n",
      "Finished upload of: gcr.io/issue-label-bot-dev/training/fairing-job:5307C7E7\n",
      "Pushed image gcr.io/issue-label-bot-dev/training/fairing-job:5307C7E7 in 4.142661200021394s.\n"
     ]
    }
   ],
   "source": [
    "builder = append.append.AppendBuilder(registry=DOCKER_REGISTRY,\n",
    "                                      base_image=cluster_builder.image_tag,\n",
    "                                      preprocessor=preprocessor)\n",
    "builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline\n",
    "Kubeflow [Pipelines](https://www.kubeflow.org/docs/pipelines/) builds reusable end-to-end machine learning workflows.\n",
    "\n",
    "Define the pipeline as a Python function. \"@kfp.dsl.pipeline\" is a required decoration including name and description properties.\n",
    "\n",
    "We define two steps for our training pipelines, including scrapping issues and training model, both of which will be executed in our built image from Kubeflow Fairing. Also, we use GPU and add GCP credentials to the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = 'gcr.io/issue-label-bot-dev/training/fairing-job:5307C7E7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='Training pipeline',\n",
    "   description='A pipeline that loads embeddings and trains a model for a github repo.'\n",
    ")\n",
    "def train_pipeline(owner, repo):\n",
    "    scrape_op = dsl.ContainerOp(\n",
    "            name='scrape issues',\n",
    "            image=target_image,\n",
    "            command=['python', 'issues_loader.py', 'save_issue_embeddings', f'--owner={owner}', f'--repo={repo}'],\n",
    "            ).set_gpu_limit(1).apply(\n",
    "                gcp.use_gcp_secret('user-gcp-sa'),\n",
    "            )\n",
    "    scrape_op.container.working_dir = '/app'\n",
    "\n",
    "    train_op = dsl.ContainerOp(\n",
    "            name='train',\n",
    "            image=target_image,\n",
    "            command=['python', 'repo_mlp.py', 'train', f'--owner={owner}', f'--repo={repo}'],\n",
    "            ).set_gpu_limit(1).apply(\n",
    "                gcp.use_gcp_secret('user-gcp-sa'),\n",
    "            )\n",
    "    train_op.container.working_dir = '/app'\n",
    "    train_op.after(scrape_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "We compile our pipeline to an intermediate representation, which is a YAML file compressed in a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = train_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution\n",
    "We upload our created pipeline, the zip file, and run it. Then, we can see the pipeline and experiments on Kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/931e8914-bf1d-4511-9456-8da86d9dcfe4\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'TrainModel'\n",
    "\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/783360fa-b54a-11e9-b9ec-42010a8e0121\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {'owner': 'kubeflow', 'repo': 'kubeflow'}\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
