{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Kubeflow Pipelines to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry point using fairing\n",
    "Kubeflow [Fairing](https://www.kubeflow.org/docs/fairing/) is a Python package that makes training and deploying machine learning models on Kubeflow easier.\n",
    "\n",
    "Here, we use the preprocessor in Kubeflow Fairing to convert a notebook to be a Python script and create an entry point for that script. After preprocessing the notebook, we can call the command in the command line like the following to run\n",
    "```Python\n",
    "$ python repo_mlp.py train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting issues_loader.ipynb to issues_loader.py\n",
      "Creating entry point for the class name IssuesLoader\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('issues_loader.py'),\n",
       " 'inference.py',\n",
       " 'embeddings.py',\n",
       " 'repo_config.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('IssuesLoader', notebook_file='issues_loader.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['embeddings.py', 'inference.py', 'repo_config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting repo_mlp.ipynb to repo_mlp.py\n",
      "Creating entry point for the class name RepoMLP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('repo_mlp.py'), 'mlp.py', 'repo_config.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP', notebook_file='repo_mlp.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'repo_config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Fairing to build docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fairing\n",
    "from fairing.builders import append\n",
    "from fairing.builders import cluster\n",
    "from fairing.deployers import job\n",
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue-label-bot-dev\n",
      "gcr.io/issue-label-bot-dev/training\n"
     ]
    }
   ],
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "print(GCP_PROJECT)\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/training'.format(GCP_PROJECT)\n",
    "print(DOCKER_REGISTRY)\n",
    "PY_VERSION = \".\".join([str(x) for x in sys.version_info[0:3]])\n",
    "BASE_IMAGE = 'python:{}'.format(PY_VERSION)\n",
    "# ucan use Dockerfile in this repo to build and use the base_image\n",
    "base_image = 'gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image\n",
    "We use builders in Kubeflow Fairing to build docker images. We use `ClusterBuilder` to builds a docker image in a Kubernetes cluster and `AppendBuilder` to append a new layer tarball. We also include `preprocessor` as a parameter to send the processed inputs to docker build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting repo_mlp.ipynb to repo_mlp.py\n",
      "Creating entry point for the class name RepoMLP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('repo_mlp.py'),\n",
       " 'mlp.py',\n",
       " 'embeddings.py',\n",
       " 'issues_loader.py',\n",
       " 'inference.py',\n",
       " 'repo_config.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP', notebook_file='repo_mlp.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'repo_config.py', 'embeddings.py', 'inference.py', 'issues_loader.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building image using cluster builder.\n",
      "Creating docker context: /tmp/fairing_context_bw4xs2lr\n",
      "Converting repo_mlp.ipynb to repo_mlp.py\n",
      "Creating entry point for the class name RepoMLP\n",
      "Waiting for fairing-builder-d8qgk to start...\n",
      "Waiting for fairing-builder-d8qgk to start...\n",
      "Waiting for fairing-builder-d8qgk to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6\n",
      "\u001b[33mWARN\u001b[0m[0006] Error while retrieving image from cache: getting image from path: open /cache/sha256:288cc7a9e121d6ff2b3053858146d7d0449c70fed521723df1d537247fe4d0d2: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0007] Checking for cached layer gcr.io/issue-label-bot-dev/training/fairing-job/cache:52b097ec9e1752ae25a961099a14457c753f759898c8727ea0fbc730a6ba877c...\n",
      "\u001b[36mINFO\u001b[0m[0007] No cached layer found for cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0007] Unpacking rootfs as cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi requires it.\n",
      "\u001b[36mINFO\u001b[0m[0155] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0170] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0170] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0170] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0170] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0170] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0170] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0214] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0214] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0214] Changed working directory to /app/\n",
      "\u001b[36mINFO\u001b[0m[0214] Creating directory /app/\n",
      "\u001b[36mINFO\u001b[0m[0214] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0214] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0214] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0214] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0214] cmd: /bin/bash\n",
      "\u001b[36mINFO\u001b[0m[0214] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "\u001b[36mINFO\u001b[0m[0214] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0247] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0247] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0247] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0247] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0247] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0248] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0269] No files were changed, appending empty layer to config. No layer added to image.\n",
      "\u001b[36mINFO\u001b[0m[0269] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0269] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0269] Pushing layer gcr.io/issue-label-bot-dev/training/fairing-job/cache:52b097ec9e1752ae25a961099a14457c753f759898c8727ea0fbc730a6ba877c to cache now\n",
      "\u001b[36mINFO\u001b[0m[0269] Taking snapshot of files...\n",
      "2019/08/08 20:40:03 existing blob: sha256:d68afcc1cf651f4109698bde41c1173e6c1968c3a15b3cd13430d6bcde8ed8b4\n",
      "2019/08/08 20:40:03 existing blob: sha256:89732bc7504122601f40269fc9ddfb70982e633ea9caf641ae45736f2846b004\n",
      "2019/08/08 20:40:04 gcr.io/issue-label-bot-dev/training/fairing-job/cache:52b097ec9e1752ae25a961099a14457c753f759898c8727ea0fbc730a6ba877c: digest: sha256:ff33c06878a7f553ac692b92ededd58e3be8bf3434a386c410726bc17fe3bfbd size: 423\n",
      "2019/08/08 20:40:04 existing blob: sha256:20352e3ed1b01dc6b9d5739c8cd952ef2962c3a44e427e24c0792c732570ae2f\n",
      "2019/08/08 20:40:04 existing blob: sha256:f401bdaa92adf9a46956b65a2d86021969297b5a5ed17b9820a54c8e6d44cf85\n",
      "2019/08/08 20:40:04 existing blob: sha256:d64ee66ee9c96d4e6d05b0c7bb3733f4f01b896f254010307610dcc90dafd728\n",
      "2019/08/08 20:40:04 existing blob: sha256:e152ee00b3dfec432f831c3add9c47dce9cd1974772890d67f5d3d0a5c8b89ae\n",
      "2019/08/08 20:40:04 existing blob: sha256:e059dd98ac7cff88cacd4e01f2f1d56af872618aac98b0aff8afdd81b9fd2c76\n",
      "2019/08/08 20:40:04 existing blob: sha256:cbeb255d6ab1895a17ac078089e678fa0189fd8204c85ad79ab69a766dbb2e8a\n",
      "2019/08/08 20:40:04 existing blob: sha256:d718e4299c08d3ee1aca64bdc55d04f86142879fa278eae6c2adaa6aee273598\n",
      "2019/08/08 20:40:04 existing blob: sha256:5b6ac7f35d3dce1cd93b9740c7342fe317b179c5ebb8d9e92069d5c671a756ae\n",
      "2019/08/08 20:40:04 existing blob: sha256:c3ae76d1ba9dbe32c0c1dfc3e72a8c0f2c27ca741784d7c3dc0229f938efba89\n",
      "2019/08/08 20:40:04 existing blob: sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2\n",
      "2019/08/08 20:40:04 existing blob: sha256:0809e577f6d6c969247b35a338c0809b324e29ee01964de6dd3c3fe4808b2a15\n",
      "2019/08/08 20:40:04 existing blob: sha256:a3842ae94f552aa6ee9e7dab71e7bcb41918eeebf74d27c9b7a647279b6775ef\n",
      "2019/08/08 20:40:04 existing blob: sha256:421e23cecfe86d864c15dc6cf8c6f41b9b8f356497d45fa786a7d3906b0fd38b\n",
      "2019/08/08 20:40:04 existing blob: sha256:d5c73556cc1e31575beb68d582444652a0e1d704e0a19bbab8565e777493ece0\n",
      "2019/08/08 20:40:04 existing blob: sha256:bfa420bf42bba88c31241000eff205efcda9c6ae42b758ad40773424e82f66a6\n",
      "2019/08/08 20:40:04 existing blob: sha256:6669e38ab1bac2750ee43e9fd769478543e5ee6866cfbbe10c5f55584681792d\n",
      "2019/08/08 20:40:04 existing blob: sha256:be8e0cd8df7cf8f297197d642acdbd141989f4235ca9decb11a86cdbe59d139b\n",
      "2019/08/08 20:40:04 existing blob: sha256:f74cae1de84e873deba13263e8409dd2d7b7e9fb94888b9ef9029b59e68b9b3e\n",
      "2019/08/08 20:40:04 existing blob: sha256:a5abf09960671b05108c96974fb37583588b286621ccd3b1229ee948d23af04e\n",
      "2019/08/08 20:40:04 existing blob: sha256:6a42806f2bf5819dcbdb874004e0032af07eda4560c12b1229588d630e401c5e\n",
      "2019/08/08 20:40:04 existing blob: sha256:f5f8ce55306cbab02d089edd0c8cd4cf402c27ceb58f98d42ac57d96b7972e57\n",
      "2019/08/08 20:40:04 existing blob: sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f\n",
      "2019/08/08 20:40:04 existing blob: sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0\n",
      "2019/08/08 20:40:04 existing blob: sha256:b855cf81cf214886646b61e0892493983f57abb8ca55ddb58ce4b74a5f80b87c\n",
      "2019/08/08 20:40:04 existing blob: sha256:2e3cac877bb2877095aa4ffad6519197728eb04865674481d1be8986b3c74ff9\n",
      "2019/08/08 20:40:04 existing blob: sha256:e4732fdd9b394aca10fbc52eb5dbd8c5a3b04e7c99044a7fd44cca2b28774f4b\n",
      "2019/08/08 20:40:04 existing blob: sha256:88462de30d91bf4e8392342ddf21c4793b4ee2b24ec8f31e7b3323615635c9d2\n",
      "2019/08/08 20:40:04 existing blob: sha256:d62ff11941adfa22f5dc2be2e4f92d3f48d3a064c27c275d4a68cfb12ff05476\n",
      "2019/08/08 20:40:04 existing blob: sha256:ae2a162faa465aa8b95a5a9bf3a3be6e05b28d5fef35b1872f21434f7dee92cc\n",
      "2019/08/08 20:40:06 pushed blob sha256:5bd644d7d4ee53d9c39545a1334d787d55e2fccc1b2ff371763f3e407588a39a\n",
      "2019/08/08 20:40:06 pushed blob sha256:dd81a869f87509229a9e346025abad411e2fd874fdbdcd29ea3b3dd5547a0b81\n",
      "2019/08/08 20:40:06 pushed blob sha256:b416e4abc6d1f206c25bbd55423af8febfee98fb5db5d63f44594b90198c077a\n",
      "2019/08/08 20:40:06 pushed blob sha256:0bae582d66663f34c3ed53ac25fe90c0c45068e7eaf11e6ec33f4e2221fce73d\n",
      "2019/08/08 20:40:06 gcr.io/issue-label-bot-dev/training/fairing-job:E9222071: digest: sha256:b7bbf2034ccbe164b59ca60a84000993b3a826c9fcf404a676942d1084220ede size: 5467\n"
     ]
    }
   ],
   "source": [
    "cluster_builder = cluster.cluster.ClusterBuilder(registry=DOCKER_REGISTRY,\n",
    "                                                 base_image=base_image,\n",
    "                                                 namespace='chunhsiang',\n",
    "                                                 preprocessor=preprocessor,\n",
    "                                                 pod_spec_mutators=[fairing.cloud.gcp.add_gcp_credentials_if_exists],\n",
    "                                                 context_source=cluster.gcs_context.GCSContextSource())\n",
    "cluster_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_os7mmd1e\n",
      "Converting repo_mlp.ipynb to repo_mlp.py\n",
      "Creating entry point for the class name RepoMLP\n",
      "repo_mlp.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'gcr.io/issue-label-bot-dev/training/fairing-job:E9222071'\n",
      "Invoking 'docker-credential-gcr' to obtain Docker credentials.\n",
      "Successfully obtained Docker credentials.\n",
      "Image successfully built in 1.9546294669999043s.\n",
      "Pushing image gcr.io/issue-label-bot-dev/training/fairing-job:50BEEED1...\n",
      "Loading Docker credentials for repository 'gcr.io/issue-label-bot-dev/training/fairing-job:50BEEED1'\n",
      "Invoking 'docker-credential-gcr' to obtain Docker credentials.\n",
      "Successfully obtained Docker credentials.\n",
      "Uploading gcr.io/issue-label-bot-dev/training/fairing-job:50BEEED1\n",
      "Layer sha256:dd81a869f87509229a9e346025abad411e2fd874fdbdcd29ea3b3dd5547a0b81 exists, skipping\n",
      "Layer sha256:f401bdaa92adf9a46956b65a2d86021969297b5a5ed17b9820a54c8e6d44cf85 exists, skipping\n",
      "Layer sha256:0809e577f6d6c969247b35a338c0809b324e29ee01964de6dd3c3fe4808b2a15 exists, skipping\n",
      "Layer sha256:f74cae1de84e873deba13263e8409dd2d7b7e9fb94888b9ef9029b59e68b9b3e exists, skipping\n",
      "Layer sha256:6669e38ab1bac2750ee43e9fd769478543e5ee6866cfbbe10c5f55584681792d exists, skipping\n",
      "Layer sha256:421e23cecfe86d864c15dc6cf8c6f41b9b8f356497d45fa786a7d3906b0fd38b exists, skipping\n",
      "Layer sha256:e152ee00b3dfec432f831c3add9c47dce9cd1974772890d67f5d3d0a5c8b89ae exists, skipping\n",
      "Layer sha256:b855cf81cf214886646b61e0892493983f57abb8ca55ddb58ce4b74a5f80b87c exists, skipping\n",
      "Layer sha256:d5c73556cc1e31575beb68d582444652a0e1d704e0a19bbab8565e777493ece0 exists, skipping\n",
      "Layer sha256:c3ae76d1ba9dbe32c0c1dfc3e72a8c0f2c27ca741784d7c3dc0229f938efba89 exists, skipping\n",
      "Layer sha256:bfa420bf42bba88c31241000eff205efcda9c6ae42b758ad40773424e82f66a6 exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:88462de30d91bf4e8392342ddf21c4793b4ee2b24ec8f31e7b3323615635c9d2 exists, skipping\n",
      "Layer sha256:be8e0cd8df7cf8f297197d642acdbd141989f4235ca9decb11a86cdbe59d139b exists, skipping\n",
      "Layer sha256:5bd644d7d4ee53d9c39545a1334d787d55e2fccc1b2ff371763f3e407588a39a exists, skipping\n",
      "Layer sha256:0bae582d66663f34c3ed53ac25fe90c0c45068e7eaf11e6ec33f4e2221fce73d exists, skipping\n",
      "Layer sha256:6a42806f2bf5819dcbdb874004e0032af07eda4560c12b1229588d630e401c5e exists, skipping\n",
      "Layer sha256:d62ff11941adfa22f5dc2be2e4f92d3f48d3a064c27c275d4a68cfb12ff05476 exists, skipping\n",
      "Layer sha256:cbeb255d6ab1895a17ac078089e678fa0189fd8204c85ad79ab69a766dbb2e8a exists, skipping\n",
      "Layer sha256:20352e3ed1b01dc6b9d5739c8cd952ef2962c3a44e427e24c0792c732570ae2f exists, skipping\n",
      "Layer sha256:ae2a162faa465aa8b95a5a9bf3a3be6e05b28d5fef35b1872f21434f7dee92cc exists, skipping\n",
      "Layer sha256:e059dd98ac7cff88cacd4e01f2f1d56af872618aac98b0aff8afdd81b9fd2c76 exists, skipping\n",
      "Layer sha256:a5abf09960671b05108c96974fb37583588b286621ccd3b1229ee948d23af04e exists, skipping\n",
      "Layer sha256:5b6ac7f35d3dce1cd93b9740c7342fe317b179c5ebb8d9e92069d5c671a756ae exists, skipping\n",
      "Layer sha256:a3842ae94f552aa6ee9e7dab71e7bcb41918eeebf74d27c9b7a647279b6775ef exists, skipping\n",
      "Layer sha256:2e3cac877bb2877095aa4ffad6519197728eb04865674481d1be8986b3c74ff9 exists, skipping\n",
      "Layer sha256:f5f8ce55306cbab02d089edd0c8cd4cf402c27ceb58f98d42ac57d96b7972e57 exists, skipping\n",
      "Layer sha256:d64ee66ee9c96d4e6d05b0c7bb3733f4f01b896f254010307610dcc90dafd728 exists, skipping\n",
      "Layer sha256:d718e4299c08d3ee1aca64bdc55d04f86142879fa278eae6c2adaa6aee273598 exists, skipping\n",
      "Layer sha256:e4732fdd9b394aca10fbc52eb5dbd8c5a3b04e7c99044a7fd44cca2b28774f4b exists, skipping\n",
      "Layer sha256:522e0eab4f2cdda215b8c53ebfd158d8c871f732a4ea72894266732cf331cac2 pushed.\n",
      "Layer sha256:4603df91ef961728fa997db7715433b41a8388dda93fea10d851f93466808c02 pushed.\n",
      "Finished upload of: gcr.io/issue-label-bot-dev/training/fairing-job:50BEEED1\n",
      "Pushed image gcr.io/issue-label-bot-dev/training/fairing-job:50BEEED1 in 4.525319297987153s.\n"
     ]
    }
   ],
   "source": [
    "builder = append.append.AppendBuilder(registry=DOCKER_REGISTRY,\n",
    "                                      base_image=cluster_builder.image_tag,\n",
    "                                      preprocessor=preprocessor)\n",
    "builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline\n",
    "Kubeflow [Pipelines](https://www.kubeflow.org/docs/pipelines/) builds reusable end-to-end machine learning workflows.\n",
    "\n",
    "Define the pipeline as a Python function. \"@kfp.dsl.pipeline\" is a required decoration including name and description properties.\n",
    "\n",
    "We define two steps for our training pipelines, including scrapping issues and training model, both of which will be executed in our built image from Kubeflow Fairing. Also, we use GPU and add GCP credentials to the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = 'gcr.io/issue-label-bot-dev/training/fairing-job:50BEEED1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='Training pipeline',\n",
    "   description='A pipeline that loads embeddings and trains a model for a github repo.'\n",
    ")\n",
    "def train_pipeline(owner, repo):\n",
    "    scrape_op = dsl.ContainerOp(\n",
    "            name='scrape issues',\n",
    "            image=target_image,\n",
    "            command=['python', 'issues_loader.py', 'save_issue_embeddings', f'--owner={owner}', f'--repo={repo}'],\n",
    "            ).set_gpu_limit(1).apply(\n",
    "                gcp.use_gcp_secret('user-gcp-sa'),\n",
    "            )\n",
    "    scrape_op.container.working_dir = '/app'\n",
    "\n",
    "    train_op = dsl.ContainerOp(\n",
    "            name='train',\n",
    "            image=target_image,\n",
    "            command=['python', 'repo_mlp.py', 'train', f'--owner={owner}', f'--repo={repo}'],\n",
    "            ).set_gpu_limit(1).apply(\n",
    "                gcp.use_gcp_secret('user-gcp-sa'),\n",
    "            )\n",
    "    train_op.container.working_dir = '/app'\n",
    "    train_op.after(scrape_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "We compile our pipeline to an intermediate representation, which is a YAML file compressed in a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = train_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution\n",
    "We upload our created pipeline, the zip file, and run it. Then, we can see the pipeline and experiments on Kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/931e8914-bf1d-4511-9456-8da86d9dcfe4\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'TrainModel'\n",
    "\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/4f2454c9-ba1d-11e9-b9ec-42010a8e0121\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {'owner': 'kubeflow', 'repo': 'kubeflow'}\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
