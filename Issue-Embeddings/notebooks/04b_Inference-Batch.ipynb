{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "This notebook illustrates the use of a utility, `InferenceWrapper.df_to_emb` that can be used to perform inference in bulk on large amounts of data.  A benchmark is provided that compares doing inference one at a time in a serial fashion and demonstrates a **10x speedup in inference time** over the previous method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of Model Artifacts\n",
    "\n",
    "### Google Cloud Storage\n",
    "\n",
    "- **model for inference** (965 MB): `https://storage.googleapis.com/issue_label_bot/model/lang_model/models_22zkdqlr/trained_model_22zkdqlr.pkl`\n",
    "\n",
    "\n",
    "- **encoder (for fine-tuning w/a classifier)** (965 MB): \n",
    "`https://storage.googleapis.com/issue_label_bot/model/lang_model/models_22zkdqlr/trained_model_encoder_22zkdqlr.pth`\n",
    "\n",
    "\n",
    "- **fastai.databunch** (27.1 GB):\n",
    "`https://storage.googleapis.com/issue_label_bot/model/lang_model/data_save.pkl`\n",
    "\n",
    "\n",
    "- **checkpointed model** (2.29 GB): \n",
    "`https://storage.googleapis.com/issue_label_bot/model/lang_model/models_22zkdqlr/best_22zkdqlr.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Minimal Model For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import InferenceWrapper, pass_through\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import Tensor, cat, device\n",
    "from torch.cuda import empty_cache\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from numpy import concatenate as cat\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# from fastai.torch_core import defaults\n",
    "# defaults.device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an `InferenceWrapper` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = InferenceWrapper(model_path='/ds/Issue-Embeddings/notebooks',\n",
    "                           model_file_name='trained_model_22zkdqlr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a test dataset\n",
    "\n",
    "The test dataset has 2,000 GitHub Issues in the below format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repo</th>\n",
       "      <th>title</th>\n",
       "      <th>title_length</th>\n",
       "      <th>body</th>\n",
       "      <th>body_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/egingric/2016-Racing-Game/i...</td>\n",
       "      <td>egingric/2016-Racing-Game</td>\n",
       "      <td>Got stuck near shortcut</td>\n",
       "      <td>25</td>\n",
       "      <td>After being blown up by the barrel, I got stuc...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/Microsoft/nodejstools/issue...</td>\n",
       "      <td>Microsoft/nodejstools</td>\n",
       "      <td>Guidance for unit test execution - How to prop...</td>\n",
       "      <td>95</td>\n",
       "      <td>What is the appropriate way to set NODE_ENV fo...</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/raphapari/dummy/issues/3</td>\n",
       "      <td>raphapari/dummy</td>\n",
       "      <td>Génération du catalogue</td>\n",
       "      <td>25</td>\n",
       "      <td>## User story xxxlnbrk - En tant que :  **gest...</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://github.com/egingric/2016-Racing-Game/i...   \n",
       "1  https://github.com/Microsoft/nodejstools/issue...   \n",
       "2        https://github.com/raphapari/dummy/issues/3   \n",
       "\n",
       "                        repo  \\\n",
       "0  egingric/2016-Racing-Game   \n",
       "1      Microsoft/nodejstools   \n",
       "2            raphapari/dummy   \n",
       "\n",
       "                                               title  title_length  \\\n",
       "0                            Got stuck near shortcut            25   \n",
       "1  Guidance for unit test execution - How to prop...            95   \n",
       "2                            Génération du catalogue            25   \n",
       "\n",
       "                                                body  body_length  \n",
       "0  After being blown up by the barrel, I got stuc...          314  \n",
       "1  What is the appropriate way to set NODE_ENV fo...          507  \n",
       "2  ## User story xxxlnbrk - En tant que :  **gest...          480  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(f'https://storage.googleapis.com/issue_label_bot/language_model_data/000000000000.csv.gz').head(8000)\n",
    "\n",
    "testdf.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Batch Inference\n",
    "\n",
    "Why Batch-Inference?  When there are a large number of issues for which you want to retrieve document embedddings, batch inference on a gpu (should be) significantly faster than on a cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Embeddings From Pre-Trained Language Model\n",
    "\n",
    "See help for `wrapper.df_to_emb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method df_to_emb in module inference:\n",
      "\n",
      "df_to_emb(dataframe:pandas.core.frame.DataFrame, bs=100) -> numpy.ndarray method of inference.InferenceWrapper instance\n",
      "    Retrieve document embeddings for a dataframe with the columns `title` and `body`.\n",
      "    Uses batching for effiecient computation, which is useful when you have many documents\n",
      "    to retrieve embeddings for. \n",
      "    \n",
      "    Paramaters\n",
      "    ----------\n",
      "    dataframe: pandas.DataFrame\n",
      "        Dataframe with columns `title` and `body`, which reprsent the Title and Body of a\n",
      "        GitHub Issue. \n",
      "    bs: int\n",
      "        batch size for doing inference.  Set this variable according to your available GPU memory.\n",
      "        The default is set to 200, which was stable on a Nvida-Tesla V-100.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    numpy.ndarray\n",
      "        An array with of shape (number of dataframe rows, 2400)\n",
      "        This numpy array represents the latent features of the GitHub issues.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> import pandas as pd\n",
      "    >>> wrapper = InferenceWrapper(model_path='/path/to/model',\n",
      "                               model_file_name='model.pkl')\n",
      "    # load 200 sample GitHub issues\n",
      "    >>> testdf = pd.read_csv(f'https://bit.ly/2GDY5NY').head(200)\n",
      "    >>> embeddings = wrapper.df_to_emb(testdf)\n",
      "    \n",
      "    >>> embeddings.shape\n",
      "    (200, 2400)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wrapper.df_to_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking inference time on 8,000 Issues\n",
    "\n",
    "#### Below is when inference is done using batching (New Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0c7ab3002c4620ab32d9f849073aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Model inference:', max=1, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 6s, sys: 21.5 s, total: 1min 27s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = wrapper.df_to_emb(testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is when inference is done one at a time (Old Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [14:48<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 42s, sys: 2min 54s, total: 15min 37s\n",
      "Wall time: 15min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare data\n",
    "test_data = [wrapper.process_dict(x)['text'] for x in testdf.to_dict(orient='rows')]\n",
    "\n",
    "emb_single = []\n",
    "for d in tqdm(test_data):\n",
    "    emb_single.append(wrapper.get_pooled_features(d).detach().cpu().numpy())\n",
    "    \n",
    "emb_single_combined = cat(emb_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "There is a **10x speedup** for inference by chunking the data into batches of similar length (to minimize padding) and passing that through the GPU.\n",
    "\n",
    "In order to get a further speed improvement we must utilize [pad_packed_sequence](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence).  We leave this a future exercise to optimize batching more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "This section tests that the embeddings retrieved from the one-at-a time approach are sufficently close to the embeddings from the batching approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(emb_single_combined, embeddings, atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
