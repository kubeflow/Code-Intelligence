{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry point using fairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('Issues_Loader.py'), 'embeddings.py', 'config.py', 'inference.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('IssuesLoader', notebook_file='Issues_Loader.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['embeddings.py', 'inference.py', 'config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('Repo_MLP.py'), 'config.py', 'mlp.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP', notebook_file='Repo_MLP.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Fairing to build docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fairing\n",
    "from fairing.builders import append\n",
    "from fairing.builders import cluster\n",
    "from fairing.deployers import job\n",
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue-label-bot-dev\n",
      "gcr.io/issue-label-bot-dev/training\n"
     ]
    }
   ],
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "print(GCP_PROJECT)\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/training'.format(GCP_PROJECT)\n",
    "print(DOCKER_REGISTRY)\n",
    "PY_VERSION = \".\".join([str(x) for x in sys.version_info[0:3]])\n",
    "BASE_IMAGE = 'python:{}'.format(PY_VERSION)\n",
    "# ucan use Dockerfile in this repo to build and use the base_image\n",
    "base_image = 'gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('Repo_MLP.py'),\n",
       " 'Issues_Loader.py',\n",
       " 'embeddings.py',\n",
       " 'config.py',\n",
       " 'mlp.py',\n",
       " 'inference.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP', notebook_file='Repo_MLP.ipynb')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'config.py', 'embeddings.py', 'inference.py', 'config.py', 'Issues_Loader.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building image using cluster builder.\n",
      "Creating docker context: /tmp/fairing_context_jxyoa4um\n",
      "Waiting for fairing-builder-h9ztc to start...\n",
      "Waiting for fairing-builder-h9ztc to start...\n",
      "Waiting for fairing-builder-h9ztc to start...\n",
      "Waiting for fairing-builder-h9ztc to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6\n",
      "\u001b[36mINFO\u001b[0m[0006] Downloading base image gcr.io/issue-label-bot-dev/ml-gpu-lite-py3.6\n",
      "\u001b[33mWARN\u001b[0m[0006] Error while retrieving image from cache: getting image from path: open /cache/sha256:007490fe99543d64363755e69ed47047c45406ae163a30fab2a7a55ec3710ceb: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0007] Checking for cached layer gcr.io/issue-label-bot-dev/training/fairing-job/cache:5abd94715d7d7183ae347a324cf72f4902c76c64083014668bdc256c7df55814...\n",
      "\u001b[36mINFO\u001b[0m[0007] No cached layer found for cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0007] Unpacking rootfs as cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi requires it.\n",
      "\u001b[36mINFO\u001b[0m[0170] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0185] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0185] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0185] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0185] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0185] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0186] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0232] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0232] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0232] Changed working directory to /app/\n",
      "\u001b[36mINFO\u001b[0m[0232] Creating directory /app/\n",
      "\u001b[36mINFO\u001b[0m[0232] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0232] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0232] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0232] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0232] cmd: /bin/bash\n",
      "\u001b[36mINFO\u001b[0m[0232] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "\u001b[36mINFO\u001b[0m[0232] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0259] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0259] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0259] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0259] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0259] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0260] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0280] No files were changed, appending empty layer to config. No layer added to image.\n",
      "\u001b[36mINFO\u001b[0m[0280] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0280] Pushing layer gcr.io/issue-label-bot-dev/training/fairing-job/cache:5abd94715d7d7183ae347a324cf72f4902c76c64083014668bdc256c7df55814 to cache now\n",
      "\u001b[36mINFO\u001b[0m[0281] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0281] Taking snapshot of files...\n",
      "2019/07/26 23:55:16 pushed blob sha256:d68afcc1cf651f4109698bde41c1173e6c1968c3a15b3cd13430d6bcde8ed8b4\n",
      "2019/07/26 23:55:16 pushed blob sha256:89732bc7504122601f40269fc9ddfb70982e633ea9caf641ae45736f2846b004\n",
      "2019/07/26 23:55:16 gcr.io/issue-label-bot-dev/training/fairing-job/cache:5abd94715d7d7183ae347a324cf72f4902c76c64083014668bdc256c7df55814: digest: sha256:ff33c06878a7f553ac692b92ededd58e3be8bf3434a386c410726bc17fe3bfbd size: 423\n",
      "2019/07/26 23:55:17 existing blob: sha256:1806ed0db5a38109f6c2b4be674030cd8b837e6ab8e5fe579358072541410777\n",
      "2019/07/26 23:55:17 existing blob: sha256:a5abf09960671b05108c96974fb37583588b286621ccd3b1229ee948d23af04e\n",
      "2019/07/26 23:55:17 existing blob: sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2\n",
      "2019/07/26 23:55:17 existing blob: sha256:5b6ac7f35d3dce1cd93b9740c7342fe317b179c5ebb8d9e92069d5c671a756ae\n",
      "2019/07/26 23:55:17 existing blob: sha256:da348912572d5e12d8c7e54b6a26417c5a2a168238c3a1c961be1ae2363eb2b8\n",
      "2019/07/26 23:55:17 existing blob: sha256:e059dd98ac7cff88cacd4e01f2f1d56af872618aac98b0aff8afdd81b9fd2c76\n",
      "2019/07/26 23:55:17 existing blob: sha256:288a4fedee6334161135d6d6c9cb91211a5f755693375bc051be4b3c2a1f4c99\n",
      "2019/07/26 23:55:17 existing blob: sha256:c700fb5e8860d28622a0214639aee6bdc260c8769bb688e9a6a84765b9350551\n",
      "2019/07/26 23:55:17 existing blob: sha256:9b879c086db6bc851a1778df6fb173a87e30a08b34810add3ffcd8b7103510d4\n",
      "2019/07/26 23:55:17 existing blob: sha256:e4732fdd9b394aca10fbc52eb5dbd8c5a3b04e7c99044a7fd44cca2b28774f4b\n",
      "2019/07/26 23:55:17 existing blob: sha256:c011ade78a64c3bef40c7a4cd87d4274a4808f7826c0b70f6679205931593cc1\n",
      "2019/07/26 23:55:17 existing blob: sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f\n",
      "2019/07/26 23:55:17 existing blob: sha256:d718e4299c08d3ee1aca64bdc55d04f86142879fa278eae6c2adaa6aee273598\n",
      "2019/07/26 23:55:17 existing blob: sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0\n",
      "2019/07/26 23:55:17 existing blob: sha256:d801c0bd10cff9d745329edcab5c0ecd7101398438ed0eadcc8ff3aa99dcad43\n",
      "2019/07/26 23:55:17 existing blob: sha256:cecc8221de0060e5aa983adaaef55f773f8c9e8cc48fe98ea66b9475a6b67e3d\n",
      "2019/07/26 23:55:17 existing blob: sha256:f401bdaa92adf9a46956b65a2d86021969297b5a5ed17b9820a54c8e6d44cf85\n",
      "2019/07/26 23:55:17 existing blob: sha256:cbeb255d6ab1895a17ac078089e678fa0189fd8204c85ad79ab69a766dbb2e8a\n",
      "2019/07/26 23:55:17 existing blob: sha256:8645605407042d2d2a5dc382ca9be54bca87eae1d1e5b97ccd4dff54d9b72207\n",
      "2019/07/26 23:55:17 existing blob: sha256:ebcc3afca65a753c949ed25905b3942bd548091a864171ce296ea4ee89344d8c\n",
      "2019/07/26 23:55:17 existing blob: sha256:4bc7ff0b2218b70a30d625db3516f88c4820879fdf232f8ad7b471cb7d13c90f\n",
      "2019/07/26 23:55:17 existing blob: sha256:9ca2ffc4c4140b428536626739a4503c91cdff680a606b8556c1e648bb113eb6\n",
      "2019/07/26 23:55:17 existing blob: sha256:1fa46105e1876c0cc24a92da5cbacd7b4a09d22675a6fd7001cce1cad8493a84\n",
      "2019/07/26 23:55:17 existing blob: sha256:0809e577f6d6c969247b35a338c0809b324e29ee01964de6dd3c3fe4808b2a15\n",
      "2019/07/26 23:55:17 existing blob: sha256:c39370e9b2806fb7774e4aef6905138a37d4e070efceca9b64830cc973c75bf4\n",
      "2019/07/26 23:55:17 existing blob: sha256:421e23cecfe86d864c15dc6cf8c6f41b9b8f356497d45fa786a7d3906b0fd38b\n",
      "2019/07/26 23:55:17 existing blob: sha256:13806a5c262364d945866dc42e584c11259483e90e367e20d7f9ff2bc9401aed\n",
      "2019/07/26 23:55:17 existing blob: sha256:d5c73556cc1e31575beb68d582444652a0e1d704e0a19bbab8565e777493ece0\n",
      "2019/07/26 23:55:17 existing blob: sha256:3339a46501ea8e79588cf6baaf5a177ea871596dfc9ffd82c93fafbdabdf97f4\n",
      "2019/07/26 23:55:17 existing blob: sha256:6669e38ab1bac2750ee43e9fd769478543e5ee6866cfbbe10c5f55584681792d\n",
      "2019/07/26 23:55:18 pushed blob sha256:8ade98f871e1aed18beb57d1949bb66d5f2ff5aedb26e6feb9ed15ad8a0346e3\n",
      "2019/07/26 23:55:18 pushed blob sha256:89cebdc3723a44d83cd5cdada7f53883a357221932fd331c181ea456baa9a0e7\n",
      "2019/07/26 23:55:18 pushed blob sha256:ccf7afd61ad7b443b14488ae4b2b80f7bf6a1adc06863ededb9dbefa653aa750\n",
      "2019/07/26 23:55:19 pushed blob sha256:3a4dedfcce244c1e0f3c77da0728cc466e1558a91df4feea37b4ed0b38fa2582\n",
      "2019/07/26 23:55:19 gcr.io/issue-label-bot-dev/training/fairing-job:36752B5C: digest: sha256:06fa159ddeae1c17a2c2b531b611574b7504c269927dcedacba28a91475a1de6 size: 5631\n"
     ]
    }
   ],
   "source": [
    "cluster_builder = cluster.cluster.ClusterBuilder(registry=DOCKER_REGISTRY,\n",
    "                                                 base_image=base_image,\n",
    "                                                 namespace='chunhsiang',\n",
    "                                                 preprocessor=preprocessor,\n",
    "                                                 pod_spec_mutators=[fairing.cloud.gcp.add_gcp_credentials_if_exists],\n",
    "                                                 context_source=cluster.gcs_context.GCSContextSource())\n",
    "cluster_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_mfh2fxiq\n",
      "Repo_MLP.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'gcr.io/issue-label-bot-dev/training/fairing-job:36752B5C'\n",
      "Invoking 'docker-credential-gcloud' to obtain Docker credentials.\n",
      "Successfully obtained Docker credentials.\n",
      "Image successfully built in 0.93903358199168s.\n",
      "Pushing image gcr.io/issue-label-bot-dev/training/fairing-job:9530A4D2...\n",
      "Loading Docker credentials for repository 'gcr.io/issue-label-bot-dev/training/fairing-job:9530A4D2'\n",
      "Invoking 'docker-credential-gcloud' to obtain Docker credentials.\n",
      "Successfully obtained Docker credentials.\n",
      "Uploading gcr.io/issue-label-bot-dev/training/fairing-job:9530A4D2\n",
      "Layer sha256:da348912572d5e12d8c7e54b6a26417c5a2a168238c3a1c961be1ae2363eb2b8 exists, skipping\n",
      "Layer sha256:c011ade78a64c3bef40c7a4cd87d4274a4808f7826c0b70f6679205931593cc1 exists, skipping\n",
      "Layer sha256:5b6ac7f35d3dce1cd93b9740c7342fe317b179c5ebb8d9e92069d5c671a756ae exists, skipping\n",
      "Layer sha256:a5abf09960671b05108c96974fb37583588b286621ccd3b1229ee948d23af04e exists, skipping\n",
      "Layer sha256:c700fb5e8860d28622a0214639aee6bdc260c8769bb688e9a6a84765b9350551 exists, skipping\n",
      "Layer sha256:d718e4299c08d3ee1aca64bdc55d04f86142879fa278eae6c2adaa6aee273598 exists, skipping\n",
      "Layer sha256:ccf7afd61ad7b443b14488ae4b2b80f7bf6a1adc06863ededb9dbefa653aa750 exists, skipping\n",
      "Layer sha256:f401bdaa92adf9a46956b65a2d86021969297b5a5ed17b9820a54c8e6d44cf85 exists, skipping\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:e059dd98ac7cff88cacd4e01f2f1d56af872618aac98b0aff8afdd81b9fd2c76 exists, skipping\n",
      "Layer sha256:9ca2ffc4c4140b428536626739a4503c91cdff680a606b8556c1e648bb113eb6 exists, skipping\n",
      "Layer sha256:6669e38ab1bac2750ee43e9fd769478543e5ee6866cfbbe10c5f55584681792d exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:ebcc3afca65a753c949ed25905b3942bd548091a864171ce296ea4ee89344d8c exists, skipping\n",
      "Layer sha256:4bc7ff0b2218b70a30d625db3516f88c4820879fdf232f8ad7b471cb7d13c90f exists, skipping\n",
      "Layer sha256:421e23cecfe86d864c15dc6cf8c6f41b9b8f356497d45fa786a7d3906b0fd38b exists, skipping\n",
      "Layer sha256:3339a46501ea8e79588cf6baaf5a177ea871596dfc9ffd82c93fafbdabdf97f4 exists, skipping\n",
      "Layer sha256:cbeb255d6ab1895a17ac078089e678fa0189fd8204c85ad79ab69a766dbb2e8a exists, skipping\n",
      "Layer sha256:1806ed0db5a38109f6c2b4be674030cd8b837e6ab8e5fe579358072541410777 exists, skipping\n",
      "Layer sha256:cecc8221de0060e5aa983adaaef55f773f8c9e8cc48fe98ea66b9475a6b67e3d exists, skipping\n",
      "Layer sha256:d5c73556cc1e31575beb68d582444652a0e1d704e0a19bbab8565e777493ece0 exists, skipping\n",
      "Layer sha256:d801c0bd10cff9d745329edcab5c0ecd7101398438ed0eadcc8ff3aa99dcad43 exists, skipping\n",
      "Layer sha256:8645605407042d2d2a5dc382ca9be54bca87eae1d1e5b97ccd4dff54d9b72207 exists, skipping\n",
      "Layer sha256:0809e577f6d6c969247b35a338c0809b324e29ee01964de6dd3c3fe4808b2a15 exists, skipping\n",
      "Layer sha256:e4732fdd9b394aca10fbc52eb5dbd8c5a3b04e7c99044a7fd44cca2b28774f4b exists, skipping\n",
      "Layer sha256:1fa46105e1876c0cc24a92da5cbacd7b4a09d22675a6fd7001cce1cad8493a84 exists, skipping\n",
      "Layer sha256:8ade98f871e1aed18beb57d1949bb66d5f2ff5aedb26e6feb9ed15ad8a0346e3 exists, skipping\n",
      "Layer sha256:13806a5c262364d945866dc42e584c11259483e90e367e20d7f9ff2bc9401aed exists, skipping\n",
      "Layer sha256:288a4fedee6334161135d6d6c9cb91211a5f755693375bc051be4b3c2a1f4c99 exists, skipping\n",
      "Layer sha256:89cebdc3723a44d83cd5cdada7f53883a357221932fd331c181ea456baa9a0e7 exists, skipping\n",
      "Layer sha256:9b879c086db6bc851a1778df6fb173a87e30a08b34810add3ffcd8b7103510d4 exists, skipping\n",
      "Layer sha256:c39370e9b2806fb7774e4aef6905138a37d4e070efceca9b64830cc973c75bf4 exists, skipping\n",
      "Layer sha256:e5d7dce2a49094cb8faf2e9355bc1e799167d24e3973ac58af43489946f22f85 pushed.\n",
      "Layer sha256:9164f0eeae84d7b151926537f4bedf638e35993a436f72377e6e899cd1abbe98 pushed.\n",
      "Finished upload of: gcr.io/issue-label-bot-dev/training/fairing-job:9530A4D2\n",
      "Pushed image gcr.io/issue-label-bot-dev/training/fairing-job:9530A4D2 in 2.8975791389821097s.\n"
     ]
    }
   ],
   "source": [
    "builder = append.append.AppendBuilder(registry=DOCKER_REGISTRY,\n",
    "                                      base_image=cluster_builder.image_tag,\n",
    "                                      preprocessor=preprocessor)\n",
    "builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_updated_image = 'gcr.io/issue-label-bot-dev/training/fairing-job:EFD117EE'\n",
    "updated_image = 'gcr.io/issue-label-bot-dev/training/fairing-job:9530A4D2'\n",
    "target_image = updated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='Training pipeline',\n",
    "   description='A pipeline that loads embeddings and trains a model for a github repo.'\n",
    ")\n",
    "def train_pipeline(owner, repo):\n",
    "    scrape_op = dsl.ContainerOp(\n",
    "            name='scrape issues',\n",
    "            image=target_image,\n",
    "            command=['python', 'Issues_Loader.py', 'save_issue_embeddings', f'--owner={owner}', f'--repo={repo}'],\n",
    "            ).apply(\n",
    "                gcp.use_gcp_secret('user-gcp-sa'),\n",
    "            )\n",
    "    scrape_op.container.working_dir = '/app'\n",
    "\n",
    "    train_op = dsl.ContainerOp(\n",
    "            name='train',\n",
    "            image=target_image,\n",
    "            command=['python', 'Repo_MLP.py', 'train', f'--owner={owner}', f'--repo={repo}'],\n",
    "            ).apply(\n",
    "                gcp.use_gcp_secret('user-gcp-sa'),\n",
    "            )\n",
    "    train_op.container.working_dir = '/app'\n",
    "    train_op.after(scrape_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = train_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/c8379d8c-60ae-4d13-a56a-7d95debe8cbd\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = 'MockupModel'\n",
    "\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/66620c12-b001-11e9-ab35-42010a8e00a3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {'owner': 'kubeflow', 'repo': 'examples'}\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
