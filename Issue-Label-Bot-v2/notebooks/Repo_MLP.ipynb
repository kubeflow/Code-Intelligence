{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "from config import Config\n",
    "from mlp import MLP\n",
    "import dill as dpickle\n",
    "import os\n",
    "import yaml\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from passlib.apps import custom_app_context as pwd_context\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "class RepoMLP(object):\n",
    "\n",
    "    def __init__(self, yaml_path=None, owner=None, repo=None, min_freq=25):\n",
    "        if not yaml_path:\n",
    "            if 'YAML_PATH' in os.environ:\n",
    "                print('yaml_path not supplied; check environment variable')\n",
    "                yaml_path = os.getenv('YAML_PATH')\n",
    "            else:\n",
    "                print('yaml_path not supplied; using the default')\n",
    "                yaml_path = 'issue_label_bot.yaml'\n",
    "        self.yaml_path = yaml_path\n",
    "        self.min_freq = min_freq # for filtering labels\n",
    "        self.clf = None\n",
    "        self.all_labels = None\n",
    "        self.load_yaml(owner, repo)\n",
    "\n",
    "    def load_yaml(self, owner, repo):\n",
    "        config = Config(self.yaml_path, owner, repo)\n",
    "        self.repo_owner = config.repo_owner\n",
    "        self.repo_name = config.repo_name\n",
    "\n",
    "        self.model_bucket_name = config.model_bucket_name\n",
    "        self.model_file = config.model_local_path\n",
    "        self.model_dest = config.model_gcs_path\n",
    "\n",
    "        self.labels_file = config.labels_local_path\n",
    "        self.labels_dest = config.labels_gcs_path\n",
    "\n",
    "        self.emb_bucket_name = config.emb_bucket_name\n",
    "        self.emb_file = config.emb_local_path\n",
    "        self.emb_dest = config.emb_gcs_path\n",
    "\n",
    "    def download_embeddings_from_gcs(self):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.emb_bucket_name)\n",
    "        blob = bucket.get_blob(self.emb_dest)\n",
    "        with open(self.emb_file, 'wb') as f:\n",
    "            blob.download_to_file(f)\n",
    "\n",
    "    def load_training_data(self):\n",
    "        self.download_embeddings_from_gcs()\n",
    "        with open(self.emb_file, 'rb') as f:\n",
    "            data = dpickle.load(f)\n",
    "\n",
    "        # filter labels\n",
    "        c = Counter()\n",
    "        for lbls in data['labels']:\n",
    "            c.update(lbls)\n",
    "        self.all_labels = [x for x in c if c[x] >= self.min_freq]\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for emb, lbls in zip(data['features'], data['labels']):\n",
    "            mask = [self.all_labels.index(x) for x in lbls if c[x] >= self.min_freq]\n",
    "            if mask == []:\n",
    "                continue\n",
    "            zer = np.zeros(len(self.all_labels))\n",
    "            zer[mask] = 1\n",
    "            y.append(zer)\n",
    "            X.append(emb)\n",
    "        return X, y\n",
    "\n",
    "    def train(self):\n",
    "        X, y = self.load_training_data()\n",
    "        self.clf = MLP()\n",
    "        self.clf.fit(X, y)\n",
    "        self.save_model()\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.clf.save_model(model_file=self.model_file)\n",
    "        # dump label columns for prediction\n",
    "        with open(self.labels_file, 'wb') as f:\n",
    "            dpickle.dump(self.all_labels, f)\n",
    "\n",
    "        self.upload_model_to_gcs()\n",
    " \n",
    "    def upload_model_to_gcs(self):\n",
    "        # upload model\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.model_bucket_name)\n",
    "        blob = bucket.blob(self.model_dest)\n",
    "        blob.upload_from_filename(self.model_file)\n",
    "\n",
    "        # upload label columns\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.model_bucket_name)\n",
    "        blob = bucket.blob(self.labels_dest)\n",
    "        blob.upload_from_filename(self.labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run locally to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaml_path not supplied; using the default\n"
     ]
    }
   ],
   "source": [
    "r = RepoMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry point using fairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('Repo_MLP.py'), 'mlp.py', 'config.py']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
